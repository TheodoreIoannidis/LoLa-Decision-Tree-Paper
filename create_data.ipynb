{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bI6hXUuexfwS"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'rm' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "fatal: destination path 'assigntools' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "# assigntools package is a course specific collection of useful tools\n",
        "! rm -fr assigntools # helps to rerun this cell witthout errors, if recloning needed\n",
        "! git clone https://github.com/kovvalsky/assigntools.git\n",
        "\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from assigntools.LoLa.read_nli import snli_jsonl2dict, sen2anno_from_nli_problems\n",
        "# from assigntools.LoLa.sen_analysis import spacy_process_sen2tok, display_doc_dep\n",
        "from nltk.tree import Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62OYO9JIPlgj"
      },
      "source": [
        "## Read data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lKU860ih-Gj",
        "outputId": "7b6515b2-1061-48c8-9f03-be0ca3c5c1fa"
      },
      "outputs": [],
      "source": [
        "# Get SNLI data\n",
        "# !wget https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
        "# !unzip snli_1.0.zip\n",
        "# !rm -r __MACOSX/ snli_1.0/*_test*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dK0AXvsgfltO",
        "outputId": "6c5c4a23-b062-4b2d-8c37-6c6df896d324"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found .json files for ['dev', 'test', 'train'] parts\n",
            "processing DEV:\t"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "10000it [00:00, 15965.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "158 problems without a gold label were ignored\n",
            "0 problems have a wrong annotator label\n",
            "9842 problems were returned\n",
            "processing TEST:\t"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "10000it [00:00, 17746.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "176 problems without a gold label were ignored\n",
            "0 problems have a wrong annotator label\n",
            "9824 problems were returned\n",
            "processing TRAIN:\t"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "550152it [00:30, 18223.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "785 problems without a gold label were ignored\n",
            "198 problems have a wrong annotator label\n",
            "549169 problems were returned\n",
            "Most common wrong annotator labels: //(198)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# takes ~1min to read and pre-process data\n",
        "# By default it reads the problems that have a gold label.\n",
        "# SNLI is dict {part: {problem_id: problem_info}}\n",
        "# S2A is dict {sentence: sentence annotation dict}\n",
        "SNLI, S2A = snli_jsonl2dict('snli_1.0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4J7GVJeWDCC",
        "outputId": "6d555bca-2ca2-4c26-d1cd-01e8e4f22b04"
      },
      "outputs": [],
      "source": [
        "# get dictionaries of sentence->annotation mappings for each split \n",
        "S2A_train = sen2anno_from_nli_problems(SNLI['train'], S2A)\n",
        "S2A_dev = sen2anno_from_nli_problems(SNLI['dev'], S2A)\n",
        "S2A_test = sen2anno_from_nli_problems(SNLI['test'], S2A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiAkHr7-Pfnw"
      },
      "source": [
        "## Create features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3G_rzLWKWZpe"
      },
      "outputs": [],
      "source": [
        "from spacy import tokens as spacy_tokens  \n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.util import ngrams\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.corpus import wordnet as wn\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "# ============================\n",
        "# Shallow Features\n",
        "# ============================\n",
        "\n",
        "def has_negation(sen, anno):\n",
        "    neg_words =  [\"no\" , \"n't\", \"not\", \"never\", \"none\", \"no one\"]\n",
        "    return 1 if any(t.lower() in neg_words for t in anno['tok']) else 0\n",
        "\n",
        "\n",
        "def giveaway_words(sen, anno):\n",
        "    \"\"\"\n",
        "    Return 1 if the sentence contains any of the giveaway words, otherwise 0.\n",
        "    (Single-sentence)\n",
        "    \"\"\"\n",
        "    giveaway_list = [\"sleep\", \"sleeping\", \"asleep\", \"slept\"]\n",
        "    return 1 if any(t.lower() in giveaway_list for t in anno['tok']) else 0\n",
        "\n",
        "\n",
        "def lexical_overlap(p_tokens, h_tokens):\n",
        "    \"\"\"\n",
        "    Compare two token lists (premise vs. hypothesis).\n",
        "    Return overlap ratio = (# shared tokens) / (# tokens in hypothesis).\n",
        "    (Pairwise)\n",
        "    \"\"\"\n",
        "    p_set = set(w.lower() for w in p_tokens)\n",
        "    h_set = set(w.lower() for w in h_tokens)\n",
        "    if len(h_set) == 0:\n",
        "        return 0.0\n",
        "    return len(p_set & h_set) / float(len(h_set))\n",
        "\n",
        "\n",
        "def length_difference(p_tokens, h_tokens):\n",
        "    \"\"\"\n",
        "    Return absolute difference in length (# tokens).\n",
        "    (Pairwise)\n",
        "    \"\"\"\n",
        "    return len(p_tokens) - len(h_tokens)\n",
        "\n",
        "\n",
        "def stopword_overlap(p_tokens, h_tokens):\n",
        "    \"\"\"\n",
        "    Return ratio of shared stopwords = (# shared stopwords) / (# stopwords in hypothesis).\n",
        "    (Pairwise)\n",
        "    \"\"\"\n",
        "    p_stop = set(w.lower() for w in p_tokens if w.lower() in STOPWORDS)\n",
        "    h_stop = set(w.lower() for w in h_tokens if w.lower() in STOPWORDS)\n",
        "    if len(h_stop) == 0:\n",
        "        return 0.0\n",
        "    return len(p_stop & h_stop) / float(len(h_stop))\n",
        "\n",
        "\n",
        "def length_ratios(p_tokens, h_tokens):\n",
        "    \"\"\"\n",
        "    Return [len_p/len_h, len_h/len_p], handling division-by-zero.\n",
        "    (Pairwise)\n",
        "    \"\"\"\n",
        "    len_p = len(p_tokens)\n",
        "    len_h = len(h_tokens)\n",
        "    if len_p == 0 or len_h == 0:\n",
        "        return [0.0, 0.0]\n",
        "    return [len_p / float(len_h), len_h / float(len_p)]\n",
        "\n",
        "\n",
        "def ngram_overlap(p_tokens, h_tokens, n=2):\n",
        "    \"\"\"\n",
        "    Returns ratio of shared n-grams of size n.\n",
        "    (Pairwise)\n",
        "    \"\"\"\n",
        "    p_ngrams = set(ngrams(p_tokens, n))\n",
        "    h_ngrams = set(ngrams(h_tokens, n))\n",
        "    if len(h_ngrams) == 0:\n",
        "        return 0.0\n",
        "    return len(p_ngrams & h_ngrams) / float(len(h_ngrams))\n",
        "\n",
        "\n",
        "\n",
        "def contains_numbers(tokens):\n",
        "    \"\"\"\n",
        "    Single-sentence helper that returns 1 if there's any digit in these tokens, else 0.\n",
        "    (But you can also use it pairwise if you do contains_numbers(p_tokens) and contains_numbers(h_tokens).)\n",
        "    \"\"\"\n",
        "    return int(any(w.isdigit() for w in tokens))\n",
        "\n",
        "\n",
        "def is_question(h_tokens):\n",
        "    \"\"\"\n",
        "    Single-sentence helper (but typically used for hypothesis).\n",
        "    Returns 1 if the sentence has a question word or a '?', else 0.\n",
        "    \"\"\"\n",
        "    question_words = {\"who\", \"what\", \"where\", \"when\", \"why\", \"how\"}\n",
        "    if any(w.lower() in question_words for w in h_tokens):\n",
        "        return 1\n",
        "    if any('?' in w for w in h_tokens):\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "\n",
        "def negation_only_in_hypothesis(p_tokens, h_tokens):\n",
        "    \"\"\"\n",
        "    Pairwise: returns 1 if the hypothesis has negation but the premise does not, else 0.\n",
        "    \"\"\"\n",
        "    neg_words = {\n",
        "        \"no\", \"n't\", \"not\", \"never\", \"none\", \n",
        "        \"no one\", \"nobody\"\n",
        "    }\n",
        "    p_has = any(w.lower() in neg_words for w in p_tokens)\n",
        "    h_has = any(w.lower() in neg_words for w in h_tokens)\n",
        "    return int(h_has and not p_has)\n",
        "\n",
        "# ============================\n",
        "# Smart Features\n",
        "# ============================\n",
        "\n",
        "wordnet_similarity_cache = {}\n",
        "\n",
        "def cached_wordnet_similarity(word1, word2):\n",
        "    \"\"\"\n",
        "    Example: a memorized approach for WordNet similarity \n",
        "    to avoid recomputing the same pair many times.\n",
        "    \"\"\"\n",
        "    if word1 not in wordnet_similarity_cache:\n",
        "        wordnet_similarity_cache[word1] = {}\n",
        "    if word2 in wordnet_similarity_cache[word1]:\n",
        "        return wordnet_similarity_cache[word1][word2]\n",
        "\n",
        "    syn1 = wn.synsets(word1)\n",
        "    syn2 = wn.synsets(word2)\n",
        "    if syn1 and syn2:\n",
        "        sim = syn1[0].wup_similarity(syn2[0]) or 0.0\n",
        "    else:\n",
        "        sim = 0.0\n",
        "    wordnet_similarity_cache[word1][word2] = sim\n",
        "    return sim\n",
        "\n",
        "def wordnet_lexical_relations(p_tokens, h_tokens, threshold=0.8):\n",
        "    \"\"\"\n",
        "    Count how many tokens in hypothesis have a WordNet similarity\n",
        "    >= threshold with some token in premise. \n",
        "    Then normalize by length of hypothesis. \n",
        "    \"\"\"\n",
        "    if not h_tokens:\n",
        "        return 0.0\n",
        "    matches = 0\n",
        "    for h in h_tokens:\n",
        "        for p in p_tokens:\n",
        "            sim = cached_wordnet_similarity(h.lower(), p.lower())\n",
        "            if sim >= threshold:\n",
        "                matches += 1\n",
        "                break  # once found a match for h, we move to next h\n",
        "    return matches / float(len(h_tokens))\n",
        "\n",
        "def cosine_similarity_feature(p_sentence, h_sentence, tfidf_vectorizer):\n",
        "    \"\"\"\n",
        "    Use a TF-IDF vectorizer to compute cosine similarity between \n",
        "    premise & hypothesis strings. \n",
        "    - 'tfidf_vectorizer' should be a fitted TfidfVectorizer object.\n",
        "    \"\"\"\n",
        "    p_vec = tfidf_vectorizer.transform([p_sentence])\n",
        "    h_vec = tfidf_vectorizer.transform([h_sentence])\n",
        "    return cosine_similarity(p_vec, h_vec)[0][0]\n",
        "\n",
        "def jaccard_similarity(p_tokens, h_tokens):\n",
        "    \"\"\"\n",
        "    Jaccard similarity = (# intersection) / (# union) of sets of tokens.\n",
        "    \"\"\"\n",
        "    p_set, h_set = set(p_tokens), set(h_tokens)\n",
        "    union = p_set | h_set\n",
        "    if not union:\n",
        "        return 0.0\n",
        "    return len(p_set & h_set) / float(len(union))\n",
        "\n",
        "def spacy_tree_similarity(p_doc, h_doc):\n",
        "    \"\"\"\n",
        "    Example of a dependency-tree-based similarity measure \n",
        "    using spaCy docs. We'll do a Jaccard on edges ignoring 'ROOT'.\n",
        "    \"\"\"\n",
        "    p_edges = set()\n",
        "    for token in p_doc:\n",
        "        if token.dep_ != 'ROOT':\n",
        "            p_edges.add((token.lemma_.lower(), token.dep_, token.head.lemma_.lower()))\n",
        "    \n",
        "    h_edges = set()\n",
        "    for token in h_doc:\n",
        "        if token.dep_ != 'ROOT':\n",
        "            h_edges.add((token.lemma_.lower(), token.dep_, token.head.lemma_.lower()))\n",
        "    \n",
        "    if not p_edges and not h_edges:\n",
        "        return 1.0  \n",
        "    if (p_edges and not h_edges) or (h_edges and not p_edges):\n",
        "        return 0.0  \n",
        "    intersection = len(p_edges & h_edges)\n",
        "    union = len(p_edges | h_edges)\n",
        "    return intersection / float(union) if union else 0.0\n",
        "\n",
        "def subject_negation_in_hypothesis_spacy(p_doc, h_doc):\n",
        "    \"\"\"\n",
        "    Returns 1 if a subject in the hypothesis that matches \n",
        "    a premise subject lemma is negated. Otherwise 0.\n",
        "    \"\"\"\n",
        "    p_subj_lemmas = set()\n",
        "    for token in p_doc:\n",
        "        if token.dep_ in (\"nsubj\", \"nsubjpass\"):\n",
        "            p_subj_lemmas.add(token.lemma_.lower())\n",
        "\n",
        "    # Check hypothesis\n",
        "    for token in h_doc:\n",
        "        if token.dep_ in (\"nsubj\", \"nsubjpass\"):\n",
        "            if token.lemma_.lower() in p_subj_lemmas:\n",
        "                # check if the subject token or any of its children has negation\n",
        "                for child in token.children:\n",
        "                    if child.dep_ == \"neg\":\n",
        "                        return 1\n",
        "    return 0\n",
        "\n",
        "def contains_adj_adv_in_hypothesis(h_tree_str):\n",
        "    \"\"\"\n",
        "    Check if the hypothesis parse tree has any adjectives or adverbs.\n",
        "    'h_tree_str' is the Penn Treebank parse (string).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        tree = Tree.fromstring(h_tree_str)\n",
        "    except:\n",
        "        return 0\n",
        "    count_adj_adv = 0\n",
        "    for subtree in tree.subtrees():\n",
        "        if subtree.label() in {\"JJ\", \"JJR\", \"JJS\", \"RB\", \"RBR\", \"RBS\"}:\n",
        "            count_adj_adv += 1\n",
        "    return int(count_adj_adv > 0)\n",
        "\n",
        "def sentiment_difference_feature(p_sentence, h_sentence, sia):\n",
        "    \"\"\"\n",
        "    Compute absolute difference of VADER sentiment compound scores.\n",
        "    'sia' is a SentimentIntensityAnalyzer instance.\n",
        "    \"\"\"\n",
        "    p_score = sia.polarity_scores(p_sentence)['compound']\n",
        "    h_score = sia.polarity_scores(h_sentence)['compound']\n",
        "    return p_score - h_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NjzKBnZ7UrRy"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     C:\\Users\\ioann\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.downloader.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "def sen2features(sen, anno):\n",
        "\n",
        "    feats = {}\n",
        "    \n",
        "    feats['tok_num'] = len(anno['tok'])\n",
        "    feats['neg_num'] = has_negation(sen, anno)\n",
        "    feats['noun_num'] = len([t for t in anno['pos'] if t == \"NNS\" or t == \"NN\"])\n",
        "    feats['plural_noun'] = len([t for t in anno['pos'] if t == \"NNS\" or t == \"NNPS\"])\n",
        "    \n",
        " \n",
        "    feats['contains_numbers'] = int(any(w.isdigit() for w in anno['tok']))\n",
        "    \n",
        "    question_words = {\"who\", \"what\", \"where\", \"when\", \"why\", \"how\"}\n",
        "    feats['is_question'] = int(\n",
        "        any(w.lower() in question_words for w in anno['tok']) or \"?\" in sen\n",
        "    )\n",
        "    \n",
        "    sentiment_scores = sia.polarity_scores(sen)\n",
        "    feats['sentiment_compound'] = sentiment_scores['compound']\n",
        "    \n",
        "    return {**feats, **anno}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fshUfLIdPsyO"
      },
      "outputs": [],
      "source": [
        "def problem2features(sen1, anno1, sen2, anno2, \n",
        "                     sen_feats=set(['tok_num', 'neg_num', 'noun_num'])):\n",
        "    \"\"\"\n",
        "    Takes two sentences (strings) and their annotations (feature dicts)\n",
        "    and returns a dictionary of feature:value pairs that characterize the \n",
        "    premise–hypothesis pair. This version merges your friend's original \n",
        "    pairwise features with your \"shallow\" and \"smart\" features.\n",
        "    \"\"\"\n",
        "\n",
        "    features = {}\n",
        "\n",
        "   \n",
        "    sen_feats = {}  # or {'neg_num','sentiment_compound'} if you want\n",
        "    sen1_feats = {f\"{k}1\": v for k, v in anno1.items() if k in sen_feats}\n",
        "    sen2_feats = {f\"{k}2\": v for k, v in anno2.items() if k in sen_feats}\n",
        "    features.update(sen1_feats)\n",
        "    features.update(sen2_feats)\n",
        "\n",
        "    \n",
        "    features['neg_diff'] = anno1['neg_num'] + anno2['neg_num']\n",
        "\n",
        "    features['token_diff'] = (\n",
        "        (anno1['tok_num'] - anno2['tok_num']) \n",
        "        / max(anno1['tok_num'], anno2['tok_num'])\n",
        "    )\n",
        "\n",
        "    features['same_tokens'] = (\n",
        "        len(set(anno1['tok']).intersection(set(anno2['tok']))) \n",
        "        / max(len(anno1['tok']), len(anno2['tok']))\n",
        "    )\n",
        "\n",
        "    features['same_pos_tags'] = (\n",
        "        len(set(anno1['pos']).intersection(set(anno2['pos']))) \n",
        "        / max(len(anno1['pos']), len(anno2['pos']))\n",
        "    )\n",
        "\n",
        "    features['giveaway'] = giveaway_words(sen2, anno2)\n",
        "\n",
        "    features['first_word_match'] = int(\n",
        "        anno1['tok'][0].lower() == anno2['tok'][0].lower()\n",
        "    )\n",
        "    features['last_word_match'] = int(\n",
        "        anno1['tok'][-1].lower() == anno2['tok'][-1].lower()\n",
        "    )\n",
        "\n",
        "    features['plural_noun_diff'] = (\n",
        "        anno1['plural_noun'] - anno2['plural_noun']\n",
        "    )\n",
        "\n",
        "    noun_exist = (\n",
        "        'NN' in anno1['pos'] and \n",
        "        'NN' in anno2['pos']\n",
        "    )\n",
        "    features['noun_match'] = 1 if (\n",
        "        noun_exist and \n",
        "        (anno1['pos'].index('NN') == anno2['pos'].index('NN'))\n",
        "    ) else 0\n",
        "\n",
        "    pronoun_exist = (\n",
        "        'PRP$' in anno1['pos'] and\n",
        "        'PRP$' in anno2['pos']\n",
        "    )\n",
        "    features['pronoun_match'] = 0\n",
        "    if pronoun_exist:\n",
        "        idx1 = anno1['pos'].index('PRP$')\n",
        "        idx2 = anno2['pos'].index('PRP$')\n",
        "        if anno1['tok'][idx1] == anno2['tok'][idx2]:\n",
        "            features['pronoun_match'] = 1\n",
        "\n",
        "    numeric = 1 if (\n",
        "        'CD' in anno1['pos'] and \n",
        "        'CD' in anno2['pos']\n",
        "    ) else 0 if (\n",
        "        'CD' in anno1['pos'] or 'CD' in anno2['pos']\n",
        "    ) else -1\n",
        "    features['numeric_match'] = 0\n",
        "    if numeric == 1:\n",
        "        idx1 = anno1['pos'].index('CD')\n",
        "        idx2 = anno2['pos'].index('CD')\n",
        "        if anno1['tok'][idx1] == anno2['tok'][idx2]:\n",
        "            features['numeric_match'] = 1\n",
        "\n",
        "  \n",
        "    p_tokens = anno1['tok']\n",
        "    h_tokens = anno2['tok']\n",
        "\n",
        "    p_doc = nlp(sen1)\n",
        "    h_doc = nlp(sen2)\n",
        "\n",
        "    features['length_difference'] = length_difference(p_tokens, h_tokens)\n",
        "\n",
        "    \n",
        "    features['lexical_overlap'] = lexical_overlap(p_tokens, h_tokens)\n",
        "\n",
        "    features['stopword_overlap'] = stopword_overlap(p_tokens, h_tokens)\n",
        "\n",
        "    len_ratios_vals = length_ratios(p_tokens, h_tokens)\n",
        "    features['len_ratio_p_h'] = len_ratios_vals[0]\n",
        "    features['len_ratio_h_p'] = len_ratios_vals[1]\n",
        "\n",
        "    features['bigram_overlap'] = ngram_overlap(p_tokens, h_tokens, n=2)\n",
        "    features['trigram_overlap'] = ngram_overlap(p_tokens, h_tokens, n=3)\n",
        "\n",
        "    p_neg = has_negation(sen1, anno1)  # how many neg words in premise\n",
        "    h_neg = has_negation(sen2, anno2)  # how many neg words in hypothesis\n",
        "    features['negation_only_in_hyp'] = int((h_neg > 0) and (p_neg == 0))\n",
        "\n",
        " \n",
        "    features['is_question_h'] = is_question(h_tokens)\n",
        "\n",
        "  \n",
        "    features['contains_numbers_p'] = contains_numbers(p_tokens)\n",
        "    features['contains_numbers_h'] = contains_numbers(h_tokens)\n",
        "\n",
        "    features['negation_in_both'] = int(p_neg > 0 and h_neg > 0)\n",
        "\n",
        "    features['wordnet_lex_rel'] = wordnet_lexical_relations(p_tokens, h_tokens)\n",
        "    features['jaccard_sim'] = jaccard_similarity(p_tokens, h_tokens)\n",
        "\n",
        "    features['spacy_tree_sim'] = spacy_tree_similarity(p_doc, h_doc)\n",
        "\n",
        "    features['subject_neg_hyp'] = subject_negation_in_hypothesis_spacy(p_doc, h_doc)\n",
        "\n",
        "    features['contains_adj_adv_in_h'] = contains_adj_adv_in_hypothesis(anno2['tree'])\n",
        "\n",
        "\n",
        "    return features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "W5BBptMfUGD-"
      },
      "outputs": [],
      "source": [
        "def problems2df(data_dict, sen2af):\n",
        "    '''\n",
        "    Read a dictionary of NLI problems {pid->prob} and\n",
        "    a dictionary of sentence annotations {sent->anno_feats}\n",
        "    and represent each problem as a set of feature-values in a DataFrame.\n",
        "    DataFrame offers an easy way of viewing and manipulating data.\n",
        "\n",
        "    Separate DataFrames are created for labels, features, and sentence pairs.\n",
        "    https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html\n",
        "    '''\n",
        "    dict_of_feats = {\n",
        "        pid: problem2features(prob['p'], sen2af[prob['p']], prob['h'], sen2af[prob['h']])\n",
        "        for pid, prob in tqdm(data_dict.items())\n",
        "    }\n",
        "\n",
        "    gold_labels = {\n",
        "        pid: [prob['g']]\n",
        "        for pid, prob in data_dict.items()\n",
        "    }\n",
        "\n",
        "    pair_df = {\n",
        "        pid: [f\"{prob['p']} ??? {prob['h']}\"]\n",
        "        for pid, prob in tqdm(data_dict.items())\n",
        "    }\n",
        "\n",
        "    feat_df = pd.DataFrame(dict_of_feats).transpose()\n",
        "    lab_df = pd.DataFrame(gold_labels).transpose()\n",
        "    pair_df = pd.DataFrame(pair_df).transpose()\n",
        "\n",
        "    lab_df = lab_df.reindex(feat_df.index)\n",
        "    pair_df = pair_df.reindex(feat_df.index)\n",
        "\n",
        "    return feat_df, lab_df, pair_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLIGFVRPXxFO",
        "outputId": "f5890e0e-6c31-4b04-e16c-2db61b58f918"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 628489/628489 [00:58<00:00, 10671.78it/s]\n",
            "100%|██████████| 12982/12982 [00:01<00:00, 9926.77it/s] \n",
            "100%|██████████| 12961/12961 [00:01<00:00, 10249.89it/s]\n"
          ]
        }
      ],
      "source": [
        "# Put together the features with the sentence annotations\n",
        "s2af = { s: sen2features(s, a) for s, a in tqdm(S2A_train.items()) }\n",
        "s2af_dev = { s: sen2features(s, a) for s, a in tqdm(S2A_dev.items()) }\n",
        "s2af_test = { s: sen2features(s, a) for s, a in tqdm(S2A_test.items()) }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUz1uh9WqFSp",
        "outputId": "01bc1472-928d-471d-b0e4-2b2a6ee11d49"
      },
      "outputs": [],
      "source": [
        "feat_df, lab_df, pair_df = problems2df(SNLI['train'], s2af)\n",
        "feat_df_dev, lab_df_dev, pair_df_dev = problems2df(SNLI['dev'], s2af_dev)\n",
        "feat_df_test, lab_df_test, pair_df_test = problems2df(SNLI['test'], s2af_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "s1eQ5ZHkrZhb",
        "outputId": "2a83a162-7b8e-4d3e-fe9c-89b1e2b9dc03"
      },
      "outputs": [],
      "source": [
        "# concatenate sentences, features and labels together\n",
        "data = pd.concat([lab_df, feat_df, pair_df], axis=1)\n",
        "data_dev = pd.concat([lab_df_dev, feat_df_dev, pair_df_dev], axis=1)\n",
        "data_test = pd.concat([lab_df_test, feat_df_test, pair_df_test], axis=1)\n",
        "\n",
        "# save to csv files\n",
        "# data.to_csv('./data_train.csv', header=True, index_label=0)\n",
        "# data_dev.to_csv('./data_dev.csv', header=True, index_label=0)\n",
        "# data_test.to_csv('./data_test.csv', header=True, index_label=0)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "13a9b329248b44ecb8721f2a9353366a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9032b4077ab84d0c966913fba92ff6a0",
            "placeholder": "​",
            "style": "IPY_MODEL_7caf99b3702d47b0b3278b194ba6a807",
            "value": " 508M/508M [00:03&lt;00:00, 147MB/s]"
          }
        },
        "4c032b77bdbe4c58b16e26c02c50f766": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "751e52ecbeb74d7aa45741cdb4874fbc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7caf99b3702d47b0b3278b194ba6a807": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9032b4077ab84d0c966913fba92ff6a0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "946b59395ddb419dbed1919c217d3a53": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5f26cd5b396415e8359fb2e85c4512c",
            "max": 507967463,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c032b77bdbe4c58b16e26c02c50f766",
            "value": 507967463
          }
        },
        "c5f26cd5b396415e8359fb2e85c4512c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf20afb41c3d472b8e55f8a8bc755543": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee815e4a488a4ea1b89e13036d69b95b",
              "IPY_MODEL_946b59395ddb419dbed1919c217d3a53",
              "IPY_MODEL_13a9b329248b44ecb8721f2a9353366a"
            ],
            "layout": "IPY_MODEL_751e52ecbeb74d7aa45741cdb4874fbc"
          }
        },
        "e44936f8b4d5480298483d0bf9976f85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eaba9540da9b433080408de73f0fe340": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee815e4a488a4ea1b89e13036d69b95b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eaba9540da9b433080408de73f0fe340",
            "placeholder": "​",
            "style": "IPY_MODEL_e44936f8b4d5480298483d0bf9976f85",
            "value": "Downloading https://huggingface.co/stanfordnlp/CoreNLP/resolve/main/stanford-corenlp-latest.zip: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
